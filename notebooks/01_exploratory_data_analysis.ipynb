{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d7ffa1",
   "metadata": {},
   "source": [
    "# Kaggle Competition - Exploratory Data Analysis\n",
    "\n",
    "**Author**: PhD-Level Data Science Solution  \n",
    "**Competition Metric**: RMSE (Root Mean Squared Error)  \n",
    "**Goal**: Comprehensive EDA to inform modeling strategy\n",
    "\n",
    "This notebook explores the training data to understand:\n",
    "- Data structure and quality\n",
    "- Target variable distribution\n",
    "- Feature characteristics and correlations\n",
    "- Outliers and anomalies\n",
    "- Feature importance signals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b77089d",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b321b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization styles\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af59632b",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04112f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_csv('../data/trainingdata.csv')\n",
    "test_df = pd.read_csv('../data/test_predictors.csv')\n",
    "sample_sub = pd.read_csv('../data/SampleSubmission.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Sample submission shape: {sample_sub.shape}\")\n",
    "print(f\"\\nNumber of features: {train_df.shape[1] - 1}\")\n",
    "print(f\"Number of training samples: {train_df.shape[0]}\")\n",
    "print(f\"Number of test samples: {test_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8187372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"Training Data:\")\n",
    "display(train_df.head())\n",
    "\n",
    "print(\"\\nTest Data:\")\n",
    "display(test_df.head())\n",
    "\n",
    "print(\"\\nSample Submission:\")\n",
    "display(sample_sub.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8787a8",
   "metadata": {},
   "source": [
    "## 3. Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db88b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values in Training Data:\")\n",
    "missing_train = train_df.isnull().sum()\n",
    "print(f\"Total missing values: {missing_train.sum()}\")\n",
    "if missing_train.sum() > 0:\n",
    "    print(missing_train[missing_train > 0])\n",
    "else:\n",
    "    print(\"No missing values detected ✓\")\n",
    "\n",
    "print(\"\\nMissing Values in Test Data:\")\n",
    "missing_test = test_df.isnull().sum()\n",
    "print(f\"Total missing values: {missing_test.sum()}\")\n",
    "if missing_test.sum() > 0:\n",
    "    print(missing_test[missing_test > 0])\n",
    "else:\n",
    "    print(\"No missing values detected ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca8d415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and info\n",
    "print(\"Training Data Info:\")\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc48270e",
   "metadata": {},
   "source": [
    "## 4. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49484bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable statistics\n",
    "y = train_df['y']\n",
    "\n",
    "print(\"Target Variable Statistics:\")\n",
    "print(y.describe())\n",
    "print(f\"\\nSkewness: {y.skew():.4f}\")\n",
    "print(f\"Kurtosis: {y.kurtosis():.4f}\")\n",
    "\n",
    "# Visualize target distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(y, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Target Value (y)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Distribution of Target Variable', fontsize=14, fontweight='bold')\n",
    "axes[0].axvline(y.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {y.mean():.2f}')\n",
    "axes[0].axvline(y.median(), color='green', linestyle='--', linewidth=2, label=f'Median: {y.median():.2f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(y, vert=True)\n",
    "axes[1].set_ylabel('Target Value (y)', fontsize=12)\n",
    "axes[1].set_title('Box Plot - Outlier Detection', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(y, dist=\"norm\", plot=axes[2])\n",
    "axes[2].set_title('Q-Q Plot - Normality Check', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for outliers using IQR method\n",
    "Q1 = y.quantile(0.25)\n",
    "Q3 = y.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = ((y < (Q1 - 1.5 * IQR)) | (y > (Q3 + 1.5 * IQR))).sum()\n",
    "print(f\"\\nNumber of outliers (IQR method): {outliers} ({100*outliers/len(y):.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9c93f7",
   "metadata": {},
   "source": [
    "## 5. Feature Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3cf951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature columns\n",
    "X = train_df.drop('y', axis=1)\n",
    "\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"\\nFeature Statistics:\")\n",
    "display(X.describe().T)\n",
    "\n",
    "# Check feature scales\n",
    "print(\"\\nFeature Value Ranges:\")\n",
    "feature_ranges = pd.DataFrame({\n",
    "    'min': X.min(),\n",
    "    'max': X.max(),\n",
    "    'range': X.max() - X.min(),\n",
    "    'mean': X.mean(),\n",
    "    'std': X.std()\n",
    "})\n",
    "display(feature_ranges.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d372ff5f",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40caa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with target\n",
    "correlations = train_df.corr()['y'].drop('y').sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 20 Features Correlated with Target:\")\n",
    "print(correlations.head(20))\n",
    "\n",
    "print(\"\\nBottom 20 Features (Negatively Correlated with Target):\")\n",
    "print(correlations.tail(20))\n",
    "\n",
    "# Visualize top correlations\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "top_corr = pd.concat([correlations.head(15), correlations.tail(15)])\n",
    "colors = ['green' if x > 0 else 'red' for x in top_corr]\n",
    "top_corr.plot(kind='barh', ax=ax, color=colors, edgecolor='black')\n",
    "ax.set_xlabel('Correlation Coefficient', fontsize=12)\n",
    "ax.set_title('Top 30 Features by Correlation with Target', fontsize=14, fontweight='bold')\n",
    "ax.axvline(0, color='black', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d664500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-feature correlation (multicollinearity check)\n",
    "feature_corr = X.corr()\n",
    "\n",
    "# Find highly correlated feature pairs\n",
    "high_corr_pairs = []\n",
    "for i in range(len(feature_corr.columns)):\n",
    "    for j in range(i+1, len(feature_corr.columns)):\n",
    "        if abs(feature_corr.iloc[i, j]) > 0.8:\n",
    "            high_corr_pairs.append({\n",
    "                'Feature1': feature_corr.columns[i],\n",
    "                'Feature2': feature_corr.columns[j],\n",
    "                'Correlation': feature_corr.iloc[i, j]\n",
    "            })\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(f\"Found {len(high_corr_pairs)} highly correlated feature pairs (|r| > 0.8):\")\n",
    "    display(pd.DataFrame(high_corr_pairs).head(20))\n",
    "else:\n",
    "    print(\"No highly correlated feature pairs found (|r| > 0.8)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c27bb7",
   "metadata": {},
   "source": [
    "## 7. Feature Importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da34662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest to get feature importance\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 Most Important Features:\")\n",
    "display(feature_importance.head(20))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "top_features = feature_importance.head(30)\n",
    "ax.barh(range(len(top_features)), top_features['Importance'], color='steelblue', edgecolor='black')\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['Feature'])\n",
    "ax.set_xlabel('Feature Importance', fontsize=12)\n",
    "ax.set_title('Top 30 Features by Random Forest Importance', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cumulative importance\n",
    "feature_importance['Cumulative'] = feature_importance['Importance'].cumsum()\n",
    "print(f\"\\nFeatures needed for 80% cumulative importance: {(feature_importance['Cumulative'] <= 0.8).sum()}\")\n",
    "print(f\"Features needed for 90% cumulative importance: {(feature_importance['Cumulative'] <= 0.9).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6774a9a",
   "metadata": {},
   "source": [
    "## 8. Key Insights and Recommendations\n",
    "\n",
    "### Data Characteristics:\n",
    "- **Small Dataset**: Only 302 training samples - requires careful validation\n",
    "- **High Dimensionality**: 112 features - feature selection/engineering critical\n",
    "- **No Missing Values**: Data is complete ✓\n",
    "\n",
    "### Modeling Strategy:\n",
    "1. **Cross-Validation**: Use 5-fold CV due to small sample size\n",
    "2. **Regularization**: Essential to prevent overfitting (Ridge, Lasso, ElasticNet)\n",
    "3. **Feature Engineering**: Create polynomial and statistical features\n",
    "4. **Ensemble Methods**: Combine multiple models to reduce variance\n",
    "5. **Gradient Boosting**: XGBoost, LightGBM, CatBoost expected to perform well\n",
    "\n",
    "### Next Steps:\n",
    "- Implement comprehensive preprocessing pipeline\n",
    "- Train diverse model ensemble\n",
    "- Use stacking/blending for final predictions\n",
    "- Monitor cross-validation scores carefully"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
