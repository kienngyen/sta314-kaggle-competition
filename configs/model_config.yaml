# Model Configuration for Kaggle Competition
# Metric: RMSE (Root Mean Squared Error)

random_state: 42
n_folds: 5
test_size: 0.2

# Traditional Models
linear_regression:
  fit_intercept: true
  
ridge:
  alpha: [0.01, 0.1, 1.0, 10.0, 100.0]
  
lasso:
  alpha: [0.001, 0.01, 0.1, 1.0, 10.0]
  
elastic_net:
  alpha: [0.001, 0.01, 0.1, 1.0]
  l1_ratio: [0.1, 0.3, 0.5, 0.7, 0.9]

# Tree-based Models
random_forest:
  n_estimators: 500
  max_depth: [10, 20, 30, null]
  min_samples_split: [2, 5, 10]
  min_samples_leaf: [1, 2, 4]
  max_features: ['sqrt', 'log2']
  
gradient_boosting:
  n_estimators: 500
  learning_rate: [0.01, 0.05, 0.1]
  max_depth: [3, 5, 7]
  min_samples_split: [2, 5]
  subsample: [0.8, 0.9, 1.0]

# XGBoost
xgboost:
  n_estimators: 1000
  learning_rate: 0.01
  max_depth: 7
  min_child_weight: 3
  subsample: 0.8
  colsample_bytree: 0.8
  gamma: 0
  reg_alpha: 0.05
  reg_lambda: 1.0
  
# LightGBM
lightgbm:
  n_estimators: 1000
  learning_rate: 0.01
  num_leaves: 31
  max_depth: -1
  min_child_samples: 20
  subsample: 0.8
  colsample_bytree: 0.8
  reg_alpha: 0.05
  reg_lambda: 1.0
  
# CatBoost
catboost:
  iterations: 1000
  learning_rate: 0.01
  depth: 7
  l2_leaf_reg: 3
  random_strength: 1
  bagging_temperature: 1

# Neural Network
neural_network:
  hidden_layers: [256, 128, 64, 32]
  dropout_rate: 0.3
  batch_size: 64
  epochs: 200
  learning_rate: 0.001
  patience: 30
  
# Stacking Ensemble
stacking:
  use_best_models: 5
  final_estimator: 'ridge'